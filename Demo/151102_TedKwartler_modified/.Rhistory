json_file <- lapply(obj, function(x) {
x[sapply(x, is.null)] <- NA
unlist(x)
})
do.call(rbind,json_file)
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
nora<-do.call(rbind.fill,obj)
nora<-do.call(rbind,obj)
nora
nora<-do.call(cbind,obj)
nora
nora$results
dim(nora)
nora[,10]
flatten(nora[,10])
unlist(nora[,10])
str(obj[[1]])
str(obj[[2]])
str(obj[[3]])
str(obj[[4]])
str(obj[[5]])
str(obj[[6]])
str(obj[[6]])
str(obj[[7]])
str(obj[[8]])
str(obj[[9]])
str(obj[[10]])
nora<-do.call(cbind,obj[,10])
nora<-do.call(cbind,obj[[,10])
nora<-do.call(cbind,obj[[,10]])
nora<-do.call(cbind,obj[[10]])
nora
str(nora)
str(nora[[1]])
unlist(nora)
unlist(nora[,1])
do.call(rbind,nora[,1])
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
nora<-do.call(cbind,obj[[10]])
nora<-do.call(rbind,nora[,1])
str(nora)
nora<-unlist(do.call(rbind,nora[,1]))
nora
nora<-do.call(cbind,obj[[10]])
nora
str(nora[1])
str(nora[2])
nora<-do.call(cbind,obj[[10]][1])
nora[1]
nora<-do.call(cbind,obj[[10]][1][[1]])
nora
nora<-do.call(cbind,obj[[10]][1]$position)
nora<-do.call(cbind,obj[[10]][[1])
nora<-do.call(cbind,obj[[10]])
library(plyr)
brenna<-lapply(nora,ldply)
?ldply
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
brenna
brenna[1]
brenna[,1]
unlist(brenna[,1])
str(brenna)
brenna
unlist(brenna[,3])
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
brenna
fd.pts<-unlist(brenna[,4])
fd.pts
fd.pts<-as.numeric(nlist(brenna[,4]))
fd.pts<-as.numeric(list(brenna[,4]))
fd.pts<-as.numeric(as.character((list(brenna[,4])))
)
fd.pts<-as.numeric(as.character((list(brenna[,4]))))
fd.pts
fd.pts<-as.numeric(as.character(unlist(brenna[,4]))))
fd.pts<-as.numeric(as.character(unlist(brenna[,4])))
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
nora<-do.call(cbind,obj[[10]])
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
brenna<-lapply(do.call(rbind),ldply)
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
fd.pts<-as.numeric(as.character(unlist(brenna[,4])))
fan.duel.pts<-as.numeric(as.character(unlist(brenna[,4])))
brenna
unlist(brenna[,2])
name<-unlist(brenna[,2])
str(name)
name<-do.call(rbind,brenna[,2])
name
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
nora<-do.call(cbind,obj[[10]])
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
brenna<-lapply(do.call(rbind),ldply)
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
fan.duel.pts<-as.numeric(as.character(unlist(brenna[,4])))
name<-do.call(rbind,brenna[,2])
meg<-do.call(rbind,nora)
meg<-do.call(cbind,nora)
meg
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
fan.duel.pts<-as.numeric(as.character(unlist(brenna[,4])))
name<-do.call(rbind,brenna[,2])
brenna[,2]
name<-do.call(rbind,brenna[,2]$text)
name<-do.call(rbind,brenna[,2][2])
name
do.call(rbind,name)
do.call(cbind,name)
name<-do.call(cbind,name[,2])
name
name<-do.call(cbind,name)
name<-do.call(rbind,brenna[,2][2])
name<-do.call(cbind,name)
name
class(name)
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
fan.duel.pts<-as.numeric(as.character(unlist(brenna[,4])))
name<-do.call(rbind,brenna[,2][2])
name<-do.call(cbind,name)
all<-data.frame(name=name[,2],projected_fanduell_pts=fan.duel.pts,salary=salary, position=position)
head(all)
all<-data.frame(name=name,projected_fanduell_pts=fan.duel.pts,salary=salary, position=position)
head(all)
name<-do.call(rbind,brenna[,2][2])
name
name<-do.call(rbind,brenna[,2])
name
name<-do.call(cbind,name)
name
name<-do.call(rbind,brenna[,2])
name
name$text
name<-do.call(cbind,brenna[,2])
name
name<-do.call(cbind,brenna[,2])
name<-do.call(rbind,brenna[,2])
brenna<-do.call(rbind,name)
brenna
brenna<-do.call(cbind,name)
name<-do.call(rbind,brenna[,2])
brenna<-do.call(cbind,name)
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
name<-do.call(rbind,brenna[,2])
nora<-do.call(cbind,name)
nora
name
str(name)
nora<-pluck(name,text)
?pluck
head(name)
str(name[,1])
str(name[1,1])
str(name[2,1])
str(name[1,2])
nora<-do.call(rbind,name[,2])
nora
json <- getURL('https://www.kimonolabs.com/api/37o069lu?apikey=Si8fhVGNJqgwWKvwKx68DBkoX6qEpTLw')
obj <- fromJSON(json)
brenna<-t(do.call(cbind,obj[[10]][1][[1]]))
position<-unlist(brenna[,1])
salary<-unlist(brenna[,3])
fan.duel.pts<-as.numeric(as.character(unlist(brenna[,4])))
name<-do.call(rbind,brenna[,2])
name<-do.call(rbind,name[,2])
all<-data.frame(name=name,projected_fanduell_pts=fan.duel.pts,salary=salary, position=position)
head(all)
Sys.date()
Sys.Date()
today<-Sys.Date()
file.name<-paste('rotowire_optimizer',today,'_week_7.csv', sep='')
write.csv(all,file.name, row.names=F)
#Load libraries
library('XML')
library('plyr')
week<-4
qb.url<-paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?player_pos=QB&WeekNumber=',week,sep='')
rb.urls<-c(paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=RB&page=1&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=RB&page=2&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=RB&page=3&WeekNumber=',week,sep=''))
wr.urls<-c(paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=WR&page=1&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=WR&page=2&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=WR&page=3&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=WR&page=4&WeekNumber=',week,sep=''))
te.urls<-c(paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=TE&page=1&WeekNumber=',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=TE&page=2&WeekNumber=',week,sep=''))
flex.urls<-c(paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=1&WeekNumber=1',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=2&WeekNumber=1',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=3&WeekNumber=1',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=4&WeekNumber=1',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=5&WeekNumber=1',week,sep=''),
paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?Player_Pos=Flex&page=6&WeekNumber=1',week,sep=''))
k.url<-paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?player_pos=K&WeekNumber=',week,sep='')
def.url<-paste('http://fftoolbox.scout.com/football/2015/weeklycheatsheets.cfm?player_pos=DEF&WeekNumber=',week,sep='')
qb.fftool <- readHTMLTable(qb.url, stringsAsFactors=F)$proj
qb.fftool$position<-'qb'
rb.fftool <-lapply(rb.urls,readHTMLTable)
rb.fftool<-sapply(rb.fftool, "[", 3)
rb.fftool<-do.call(rbind,rb.fftool)
rb.fftool<-na.omit(rb.fftool)
rb.fftool$position<-'rb'
wr.fftool <-lapply(wr.urls,readHTMLTable)
wr.fftool<-sapply(wr.fftool, "[", 3)
wr.fftool<-do.call(rbind,wr.fftool)
wr.fftool<-na.omit(wr.fftool)
wr.fftool$position<-'wr'
te.fftool <-lapply(te.urls,readHTMLTable)
te.fftool<-sapply(te.fftool, "[", 3)
te.fftool<-do.call(rbind,te.fftool)
te.fftool<-na.omit(te.fftool)
te.fftool$position<-'te'
flex.fftool <-lapply(flex.urls,readHTMLTable)
flex.fftool<-sapply(flex.fftool, "[", 3)
flex.fftool<-do.call(rbind,flex.fftool)
flex.fftool<-na.omit(flex.fftool)
flex.fftool$position<-'flex'
k.fftool <- readHTMLTable(k.url, stringsAsFactors=F)$proj
k.fftool$position<-'k'
def.fftool <- readHTMLTable(def.url, stringsAsFactors=F)$proj
def.fftool$position<-'def'
#All
all<-rbind.fill(qb.fftool,rb.fftool,wr.fftool,te.fftool,flex.fftool,k.fftool,def.fftool)
#get week
file.name<-paste('fftoolbox_com_week_',week,'.csv', sep='')
#save
write.csv(all,file.name,row.names=F)
wr.fftool
wr.fftool<-sapply(wr.fftool, "[", 4)
wr.fftool<-sapply(wr.fftool, "[", 2)
wr.fftool<-sapply(wr.fftool, "[", 1)
wr.fftool
#FFTODAY
#Ted
week<-1
position<-'qb'
Pos.ID<-10 #10 =qb, 20=rb, 30=wr,40=TE,80=K,XX=DEF
qb.ffto<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=1',sep='') #FFTOday
qb.cbs<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=26943',sep='') #CBS
qb.espn<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=26955',sep='') #ESPN
qb.yahoo<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=17',sep='') #Yahoo
qb.nfl<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=143908',sep='') #nfl.com
qb.fppc<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=107437',sep='') ##FFPC
qb.nffc<-paste('http://www.fftoday.com/rankings/playerwkproj.php?Season=2015&GameWeek=',week,'&PosID=',Pos.ID,'&LeagueID=5',sep='') #NFFC
qb.ffto.stats<-readHTMLTable(qb.ffto)[11]
qb.ffto.stats<-do.call(rbind,qb.ffto.stats)
qb.ffto.stats <- qb.ffto.stats[-1, ]
qb.ffto.stats <- qb.ffto.stats[,-1]
colnames(qb.ffto.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','ffto_points')
#qb.ffto.stats$qb<-position
qb.ffto.stats$name<-substring(qb.ffto.stats$name, 3)
qb.cbs.stats<-readHTMLTable(qb.cbs)[11]
qb.cbs.stats<-do.call(rbind,qb.cbs.stats)
qb.cbs.stats <- qb.cbs.stats[-1, ]
qb.cbs.stats <- qb.cbs.stats[,-1]
colnames(qb.cbs.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','cbs_points')
#qb.cbs.stats$qb<-position
qb.cbs.stats$name<-substring(qb.cbs.stats$name, 3)
qb.espn.stats<-readHTMLTable(qb.espn)[11]
qb.espn.stats<-do.call(rbind,qb.espn.stats)
qb.espn.stats <- qb.espn.stats[-1, ]
qb.espn.stats <- qb.espn.stats[,-1]
colnames(qb.espn.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','espn_points')
#qb.espn.stats$qb<-position
qb.espn.stats$name<-substring(qb.espn.stats$name, 3)
qb.yahoo.stats<-readHTMLTable(qb.yahoo)[11]
qb.yahoo.stats<-do.call(rbind,qb.yahoo.stats)
qb.yahoo.stats <- qb.yahoo.stats[-1, ]
qb.yahoo.stats <- qb.yahoo.stats[,-1]
colnames(qb.yahoo.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','yahoo_points')
#qb.yahoo.stats$qb<-position
qb.yahoo.stats$name<-substring(qb.yahoo.stats$name, 3)
qb.nfl.stats<-readHTMLTable(qb.nfl)[11]
qb.nfl.stats<-do.call(rbind,qb.nfl.stats)
qb.nfl.stats <- qb.nfl.stats[-1, ]
qb.nfl.stats <- qb.nfl.stats[,-1]
colnames(qb.nfl.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','nfl_points')
#qb.nfl.stats$qb<-position
qb.nfl.stats$name<-substring(qb.nfl.stats$name, 3)
qb.fppc.stats<-readHTMLTable(qb.fppc)[11]
qb.fppc.stats<-do.call(rbind,qb.fppc.stats)
qb.fppc.stats <- qb.fppc.stats[-1, ]
qb.fppc.stats <- qb.fppc.stats[,-1]
colnames(qb.fppc.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','fppc_points')
#qb.fppc.stats$qb<-position
qb.fppc.stats$name<-substring(qb.fppc.stats$name, 3)
qb.nffc.stats<-readHTMLTable(qb.nffc)[11]
qb.nffc.stats<-do.call(rbind,qb.nffc.stats)
qb.nffc.stats <- qb.nffc.stats[-1, ]
qb.nffc.stats <- qb.nffc.stats[,-1]
colnames(qb.nffc.stats) <- c('name','team','opponent','completions','attempts','yard','touchdowns','interceptions','rushing_attempts','rushing_yards','rushing_TDs','fppc_points')
#qb.nffc.stats$qb<-position
qb.nffc.stats$name<-substring(qb.nffc.stats$name, 3)
#Create a small dataframe of stringstext.df<-data.frame(id=seq(1:5),                     text1=c('Matt loves pizza and eats it a lot','Bob is good at basketball',                            'Conor is with his family!','i need coffee', 'lots of coffee'),                     text2=c('and ice cream too','but not ping pong','They are wild.',' to stay awake','because I am tired'))
text.df<-data.frame(id=seq(1:5),
text1=c('Matt loves pizza and eats it a lot','Bob is good at basketball',
'Conor is with his family!','i need coffee', 'lots of coffee'),
text2=c('and ice cream too','but not ping pong','They are wild.',' to stay awake','because I am tired'))
sub('pizza','ice cream',text.df[1,2], ignore.case=F)
text.df
sub('Matt',text.df[,2], ignore.case=F)
sub('Matt','Josh',text.df[,2], ignore.case=F)
text.df<-data.frame(id=seq(1:5),
text1=c('Matt loves pizza and eats pizza a lot','Bob is good at basketball',
'Conor is with his family!','Matt needs coffee', 'lots of coffee'),
text2=c('and ice cream too','but not ping pong','They are wild.',' to stay awake','because I am tired'))
sub('Matt','Josh',text.df[,2], ignore.case=F)
sub('Matt','Josh',text.df[,2], ignore.case=F)
gsub('pizza','ice cream', text.df[1,2], ignore.case=F)
text.df
gsub('coffee','whiskey', text.df[1,2], ignore.case=F)
gsub('coffee','whiskey', text.df[,2], ignore.case=F)
text.df
gsub('Matt','', text.df[1,2],ignore.case=F)
gsub('Matt','', text.df,ignore.case=F)
gsub('Matt','', text.df[1,2],ignore.case=F)
text.df
patterns<-c('pizza','coffee','ice cream')
replacements<-c('cookies','beer','brussel sprouts')
mgsub(patterns,replacements,text.df[,2])
library(qdap)
mgsub(patterns,replacements,text.df[,2])
mgsub(patterns,replacements,text.df[,3])
text.df$combined<-paste(text.df$text1,text.df$text2, sep=' ')
text.df$combined
text.df[,4]
substr(text.df$text1,1,1)
?substr
substr(text.df$text1,1,2)
new.text<-c('@toody444 The MSP-NAS non-stop service is only on Saturday. *MR',   '@daryjwalters We know how frustrating this must be.  Again, my apologies! *SB')
strsplit(new.text,"[*]")
ex.function(2)
ex.function<-function(x){  x<-x^2  x<-x+1  return(x)}
ex.function<-function(x){
x<-x^2
x<-x+1
return(x)
}
ex.function(2)
install.packages(c('tm', 'stringi','ggplot2','ggthemes','wordcloud','RColorBrewer',
'plyr','stringr','topicmodels','portfolio','openNLP', 'qdap'),
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
install.packages('openNLPmodels.en', repos = "http://datacube.wu.ac.at/", type = "source")
library(tm)
library(stringi)
library(ggplot2)
library(ggthemes)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(stringr)
library(topicmodels)
library(portfolio)
library(openNLP)
library(qdap)
library(qdap)
text1<-'i love St Peters University'
text2<-'this lecture is very good'
text3<-'data science is hard I like it a little'
polarity(text1)
polarity(text2)
polarity(text3)
text4<-'data science is hard'
polarity(text4)
text1<-'i love St Peters University'
text2<-'this lecture is good'
text3<-'this lecture is very good'
text4<-'data science is hard I like it a little'
text5<-'data science is hard'
polarity(text1)
polarity(text2)
polarity(text3)
polarity(text4)
polarity(text5)
?polarity
setwd('/Users/ted/Desktop/St Peters')
#libraries
library(qdap)
library(tm)
library(topicmodels)
library(portfolio)
#options, functions
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')
#try to lower function
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
clean.corpus<-function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords, custom.stopwords)
return(corpus)
}
#Create custom stop words
custom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'amp', 'chardonnay')
#Bigram token maker
bigram.tokenizer <-function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#Bring in data
text<-read.csv(file="coffee.csv", head=TRUE)
#Create a clean corpus
corpus <- VCorpus(DataframeSource(data.frame(text$text)))
corpus <-clean.corpus(corpus)
#Make a DTM
dtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- CTM(dtm.new, k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
#Score each tweet's probability for the topic models then add the topic words to the df as headers
scoring<-posterior(topic.model)
scores<-scoring$topics
scores<-as.data.frame(scores)
colnames(scores)<-c(t1,t2,t3,t4,t5)
#The max probability of each tweet classifies the tweet document
topics.text<-as.data.frame(cbind(row.names(scores),apply(scores,1,function(x) names(scores)[which(x==max(x))])))
#Apply the subjective lexicon scoring function
#library(qdap)
sentiment.score<-polarity(text$text)#, pos,neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score[[1]][3], doc.length)
names(all)[2]<-paste("topic")
names(all)[8]<-paste("sentiment")
names(all)[9]<-paste("length")
all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
#End
text<-read.csv(file="chardonnay2.csv", head=TRUE)
#Create a clean corpus
corpus <- VCorpus(DataframeSource(data.frame(text$text)))
corpus <-clean.corpus(corpus)
#Make a DTM
dtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- CTM(dtm.new, k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
#Score each tweet's probability for the topic models then add the topic words to the df as headers
scoring<-posterior(topic.model)
scores<-scoring$topics
scores<-as.data.frame(scores)
colnames(scores)<-c(t1,t2,t3,t4,t5)
#The max probability of each tweet classifies the tweet document
topics.text<-as.data.frame(cbind(row.names(scores),apply(scores,1,function(x) names(scores)[which(x==max(x))])))
#Apply the subjective lexicon scoring function
#library(qdap)
sentiment.score<-polarity(text$text)#, pos,neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score[[1]][3], doc.length)
names(all)[2]<-paste("topic")
names(all)[8]<-paste("sentiment")
names(all)[9]<-paste("length")
all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
#End
